{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import datetime\n",
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORT_COUNT = 5000000\n",
    "TEST_COUNT = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is: 71926\n"
     ]
    }
   ],
   "source": [
    "# Generate random seed\n",
    "#myrand=np.random.randint(1, 99999 + 1)\n",
    "myrand=71926\n",
    "np.random.seed(myrand)\n",
    "tf.random.set_seed(myrand)\n",
    "print(\"Random seed is:\",myrand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG_OUTPUT_FILENAME=\"mersenne_twist_xorshifter.txt\"\n",
    "df = np.genfromtxt(RNG_OUTPUT_FILENAME,delimiter=', ',dtype='uint64', max_rows=IMPORT_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates how many bits are in the output.\n",
    "BIT_WIDTH=np.ceil(np.log2(np.amax(df))).astype(int)\n",
    "# convert the generated numbers to binary sequences\n",
    "df_as_bits = np.concatenate(((df[:, 0][:,None] & (1 << np.arange(BIT_WIDTH,dtype='uint64')) > 0).astype(int), (df[:, 1][:,None] & (1 << np.arange(BIT_WIDTH,dtype='uint64')) > 0).astype(int)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data frames\n",
    "indicies = np.arange(df_as_bits.shape[0],dtype='uint64')\n",
    "np.random.shuffle(indicies)\n",
    "df_as_bits=df_as_bits[indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data into inputs and outputs\n",
    "y = df_as_bits[:,32:]\n",
    "X = df_as_bits[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test data\n",
    "X_train = X[TEST_COUNT:]\n",
    "X_test = X[:TEST_COUNT]\n",
    "y_train = y[TEST_COUNT:]\n",
    "y_test = y[:TEST_COUNT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    LOSS=\"binary_crossentropy\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(640, activation='relu',input_shape=[X.shape[1]] ))\n",
    "    model.add(Dense(y.shape[1], activation='sigmoid'))\n",
    "    \n",
    "    opt = keras.optimizers.Nadam(\n",
    "        learning_rate=hp.Float(\"learning_rate\", 10**(-5), 10**(-2),sampling=\"log\"),\n",
    "        epsilon=hp.Float(\"epsilon\",1e-7,1e-4,sampling=\"log\"),\n",
    "        beta_1=hp.Float(\"beta_1\",.8,.99999,sampling=\"reverse_log\"),\n",
    "        beta_2=hp.Float(\"beta_2\",.8,.99999,sampling=\"reverse_log\"),\n",
    "        )\n",
    "    model.compile(optimizer=opt, loss=LOSS,metrics=['binary_accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define callback functions\n",
    "stopEarly = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_binary_accuracy', min_delta=.001, patience=10, verbose=0, mode='auto', restore_best_weights=False\n",
    ")\n",
    "\n",
    "log_dir = \"tmp/hyperparameters_tempering/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,profile_batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a short set from the training for hyper parameter tuning\n",
    "X_train_short= X_train[:1000000]\n",
    "y_train_short= y_train[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 Complete [00h 07m 46s]\n",
      "val_binary_accuracy: 0.9846875071525574\n",
      "\n",
      "Best val_binary_accuracy So Far: 1.0\n",
      "Total elapsed time: 02h 32m 18s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in .\\tmp/tempering\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000015D1A2E9F10>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.008718330030541725\n",
      "epsilon: 5.751763055109973e-06\n",
      "beta_1: 0.8989345278917303\n",
      "beta_2: 0.9074604423190239\n",
      "Score: 1.0\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0038593516478438684\n",
      "epsilon: 7.563276785784439e-07\n",
      "beta_1: 0.8822897791929948\n",
      "beta_2: 0.8845332809911142\n",
      "Score: 0.9999468922615051\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.01\n",
      "epsilon: 7.632069047112443e-06\n",
      "beta_1: 0.9099285589191526\n",
      "beta_2: 0.8000000000000002\n",
      "Score: 0.9998999834060669\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.01\n",
      "epsilon: 4.38555603367085e-06\n",
      "beta_1: 0.8000000000000002\n",
      "beta_2: 0.8847060147913317\n",
      "Score: 0.9881437420845032\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001950816046384373\n",
      "epsilon: 2.3767858312959852e-07\n",
      "beta_1: 0.820501996538249\n",
      "beta_2: 0.9205997416654469\n",
      "Score: 0.9847218990325928\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001119404496403103\n",
      "epsilon: 1e-07\n",
      "beta_1: 0.8000000000000002\n",
      "beta_2: 0.8000000000000002\n",
      "Score: 0.9846875071525574\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.01\n",
      "epsilon: 0.0001\n",
      "beta_1: 0.8000000000000002\n",
      "beta_2: 0.8000000000000002\n",
      "Score: 0.984403133392334\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0008093262370610817\n",
      "epsilon: 3.824434879437115e-06\n",
      "beta_1: 0.9757233918257775\n",
      "beta_2: 0.9680146121732406\n",
      "Score: 0.9830906391143799\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0006690821551921309\n",
      "epsilon: 1.1901437267313975e-06\n",
      "beta_1: 0.9271252249580898\n",
      "beta_2: 0.8373223590078801\n",
      "Score: 0.9735312461853027\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.01\n",
      "epsilon: 4.3781014592718205e-06\n",
      "beta_1: 0.8000000000000002\n",
      "beta_2: 0.8000000000000002\n",
      "Score: 0.9690812230110168\n",
      "CPU times: total: 23h 31min 2s\n",
      "Wall time: 2h 32min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tuning the hyper parameters\n",
    "tuner = kt.tuners.bayesian.BayesianOptimization(build_model,'val_binary_accuracy',25,project_name=\"tmp/tempering\", seed=myrand)\n",
    "\n",
    "while tuner.remaining_trials>0:\n",
    "    try:\n",
    "        tuner.search(X_train_short, y_train_short,batch_size=2048, epochs=100, validation_data=(X_test,y_test),callbacks=[stopEarly,tensorboard_callback])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 640)               21120     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                20512     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,632\n",
      "Trainable params: 41,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.008718330030541725,\n",
       " 'epsilon': 5.751763055109973e-06,\n",
       " 'beta_1': 0.8989345278917303,\n",
       " 'beta_2': 0.9074604423190239}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "# use the best model for training\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "best_hps.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X = X_train, Y=y_train, epochs=100, batch_size=2048,verbose=1, log_dir = \"tmp/dense_model/\"):\n",
    "    log_dir += datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,profile_batch=0)\n",
    "    model.fit(X, Y, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size,callbacks=[tensorboard_callback],verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.4978 - binary_accuracy: 0.6624 - val_loss: 0.3570 - val_binary_accuracy: 0.7718\n",
      "Epoch 2/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.2782 - binary_accuracy: 0.8192 - val_loss: 0.2282 - val_binary_accuracy: 0.8524\n",
      "Epoch 3/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.1687 - binary_accuracy: 0.9019 - val_loss: 0.1136 - val_binary_accuracy: 0.9372\n",
      "Epoch 4/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0890 - binary_accuracy: 0.9470 - val_loss: 0.0756 - val_binary_accuracy: 0.9517\n",
      "Epoch 5/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0673 - binary_accuracy: 0.9567 - val_loss: 0.0559 - val_binary_accuracy: 0.9656\n",
      "Epoch 6/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0514 - binary_accuracy: 0.9670 - val_loss: 0.0483 - val_binary_accuracy: 0.9677\n",
      "Epoch 7/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0468 - binary_accuracy: 0.9682 - val_loss: 0.0466 - val_binary_accuracy: 0.9682\n",
      "Epoch 8/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0453 - binary_accuracy: 0.9686 - val_loss: 0.0443 - val_binary_accuracy: 0.9687\n",
      "Epoch 9/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0447 - binary_accuracy: 0.9687 - val_loss: 0.0442 - val_binary_accuracy: 0.9686\n",
      "Epoch 10/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0445 - binary_accuracy: 0.9687 - val_loss: 0.0449 - val_binary_accuracy: 0.9682\n",
      "Epoch 11/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0443 - binary_accuracy: 0.9689 - val_loss: 0.0438 - val_binary_accuracy: 0.9696\n",
      "Epoch 12/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0388 - binary_accuracy: 0.9748 - val_loss: 0.0297 - val_binary_accuracy: 0.9813\n",
      "Epoch 13/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0267 - binary_accuracy: 0.9829 - val_loss: 0.0243 - val_binary_accuracy: 0.9846\n",
      "Epoch 14/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0199 - binary_accuracy: 0.9889 - val_loss: 0.0158 - val_binary_accuracy: 0.9903\n",
      "Epoch 15/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0120 - binary_accuracy: 0.9951 - val_loss: 0.0074 - val_binary_accuracy: 0.9979\n",
      "Epoch 16/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0058 - binary_accuracy: 0.9984 - val_loss: 0.0028 - val_binary_accuracy: 0.9996\n",
      "Epoch 17/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0026 - binary_accuracy: 0.9995 - val_loss: 0.0018 - val_binary_accuracy: 0.9996\n",
      "Epoch 18/30\n",
      "2437/2437 [==============================] - 6s 3ms/step - loss: 0.0016 - binary_accuracy: 0.9997 - val_loss: 0.0010 - val_binary_accuracy: 0.9998\n",
      "Epoch 19/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 0.0011 - binary_accuracy: 0.9998 - val_loss: 9.1433e-04 - val_binary_accuracy: 0.9999\n",
      "Epoch 20/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 8.3665e-04 - binary_accuracy: 0.9998 - val_loss: 5.3904e-04 - val_binary_accuracy: 0.9999\n",
      "Epoch 21/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 6.3382e-04 - binary_accuracy: 0.9999 - val_loss: 5.8941e-04 - val_binary_accuracy: 0.9999\n",
      "Epoch 22/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 4.8124e-04 - binary_accuracy: 0.9999 - val_loss: 2.9243e-04 - val_binary_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 3.4575e-04 - binary_accuracy: 1.0000 - val_loss: 2.1832e-04 - val_binary_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 2.4041e-04 - binary_accuracy: 1.0000 - val_loss: 1.7322e-04 - val_binary_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 1.6998e-04 - binary_accuracy: 1.0000 - val_loss: 1.2867e-04 - val_binary_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 1.3011e-04 - binary_accuracy: 1.0000 - val_loss: 1.0538e-04 - val_binary_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 1.0135e-04 - binary_accuracy: 1.0000 - val_loss: 9.5264e-05 - val_binary_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 8.2453e-05 - binary_accuracy: 1.0000 - val_loss: 8.7882e-05 - val_binary_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 7.0708e-05 - binary_accuracy: 1.0000 - val_loss: 7.8415e-05 - val_binary_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "2437/2437 [==============================] - 7s 3ms/step - loss: 6.2305e-05 - binary_accuracy: 1.0000 - val_loss: 6.3016e-05 - val_binary_accuracy: 1.0000\n",
      "CPU times: user 5min 47s, sys: 1min 8s, total: 6min 56s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my_model_trained = train_model(model, epochs=30,log_dir = \"tmp/mt_tempering/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 1ms/step - loss: 6.3016e-05 - binary_accuracy: 1.0000\n",
      "test loss: 0.000063, test acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"test loss: %f, test acc: %s\" % tuple(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mt_tempering_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c979506cc9afdf8aa226c02a415aa6e55fb0b686ab6aba9022c7c04b1ee1b54e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
